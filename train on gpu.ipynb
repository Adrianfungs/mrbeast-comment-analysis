{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: kaggle.json: No such file or directory\n",
      "chmod: /Users/juno_fung/.kaggle/kaggle.json: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "\n",
    "\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-15T14:11:27.144984Z",
     "iopub.status.busy": "2025-11-15T14:11:27.144645Z",
     "iopub.status.idle": "2025-11-15T14:11:48.004227Z",
     "shell.execute_reply": "2025-11-15T14:11:48.003066Z",
     "shell.execute_reply.started": "2025-11-15T14:11:27.144952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juno_fung/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:11:48.006748Z",
     "iopub.status.busy": "2025-11-15T14:11:48.005918Z",
     "iopub.status.idle": "2025-11-15T14:11:48.016676Z",
     "shell.execute_reply": "2025-11-15T14:11:48.015514Z",
     "shell.execute_reply.started": "2025-11-15T14:11:48.006715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:11:48.018193Z",
     "iopub.status.busy": "2025-11-15T14:11:48.017650Z",
     "iopub.status.idle": "2025-11-15T14:11:48.121198Z",
     "shell.execute_reply": "2025-11-15T14:11:48.120200Z",
     "shell.execute_reply.started": "2025-11-15T14:11:48.018164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Achieving million views in days is dangerous</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many people here want to participate in su...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mrbeast is slowly turning into mrjigsaw</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>genuinely can't believe how dystopian this is</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have of the worlds smartest people compete in ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Sentiment\n",
       "0       Achieving million views in days is dangerous  Positive\n",
       "1  How many people here want to participate in su...   Neutral\n",
       "2            Mrbeast is slowly turning into mrjigsaw  Negative\n",
       "3      genuinely can't believe how dystopian this is  Negative\n",
       "4  Have of the worlds smartest people compete in ...   Neutral"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"Skywork/Skywork-Reward-V2-Qwen3-1.7B\"\n",
    "\n",
    "txt = pd.read_csv(\"sentiment_analysis_dataset.csv\", on_bad_lines='skip')\n",
    "\n",
    "txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:11:48.123236Z",
     "iopub.status.busy": "2025-11-15T14:11:48.122964Z",
     "iopub.status.idle": "2025-11-15T14:11:48.135563Z",
     "shell.execute_reply": "2025-11-15T14:11:48.134605Z",
     "shell.execute_reply.started": "2025-11-15T14:11:48.123212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = list(txt.iloc[:, 0].astype(\"str\"))\n",
    "labels = list(txt.iloc[:, 1].str.strip())\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:11:48.136808Z",
     "iopub.status.busy": "2025-11-15T14:11:48.136531Z",
     "iopub.status.idle": "2025-11-15T14:11:48.165563Z",
     "shell.execute_reply": "2025-11-15T14:11:48.164514Z",
     "shell.execute_reply.started": "2025-11-15T14:11:48.136788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text, labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:12:44.938231Z",
     "iopub.status.busy": "2025-11-15T14:12:44.937766Z",
     "iopub.status.idle": "2025-11-15T14:12:58.143464Z",
     "shell.execute_reply": "2025-11-15T14:12:58.137554Z",
     "shell.execute_reply.started": "2025-11-15T14:12:44.938183Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:629\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    627\u001b[39m     progress.update(progress_bytes)\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m download_files(\n\u001b[32m    630\u001b[39m     xet_download_info,\n\u001b[32m    631\u001b[39m     endpoint=connection_info.endpoint,\n\u001b[32m    632\u001b[39m     token_info=(connection_info.access_token, connection_info.expiration_unix_epoch),\n\u001b[32m    633\u001b[39m     token_refresher=token_refresher,\n\u001b[32m    634\u001b[39m     progress_updater=[progress_updater],\n\u001b[32m    635\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m      4\u001b[39m     MODEL_NAME,\n\u001b[32m      5\u001b[39m     num_labels = \u001b[32m3\u001b[39m,\n\u001b[32m      6\u001b[39m     ignore_mismatched_sizes=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m model.to(device)\n\u001b[32m     11\u001b[39m train_encodings = tokenizer(X_train, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    605\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    606\u001b[39m     )\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/transformers/modeling_utils.py:4900\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4891\u001b[39m     gguf_file\n\u001b[32m   4892\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4893\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4894\u001b[39m ):\n\u001b[32m   4895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4896\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4897\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4898\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4900\u001b[39m checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n\u001b[32m   4901\u001b[39m     pretrained_model_name_or_path=pretrained_model_name_or_path,\n\u001b[32m   4902\u001b[39m     subfolder=subfolder,\n\u001b[32m   4903\u001b[39m     variant=variant,\n\u001b[32m   4904\u001b[39m     gguf_file=gguf_file,\n\u001b[32m   4905\u001b[39m     from_tf=from_tf,\n\u001b[32m   4906\u001b[39m     from_flax=from_flax,\n\u001b[32m   4907\u001b[39m     use_safetensors=use_safetensors,\n\u001b[32m   4908\u001b[39m     cache_dir=cache_dir,\n\u001b[32m   4909\u001b[39m     force_download=force_download,\n\u001b[32m   4910\u001b[39m     proxies=proxies,\n\u001b[32m   4911\u001b[39m     local_files_only=local_files_only,\n\u001b[32m   4912\u001b[39m     token=token,\n\u001b[32m   4913\u001b[39m     user_agent=user_agent,\n\u001b[32m   4914\u001b[39m     revision=revision,\n\u001b[32m   4915\u001b[39m     commit_hash=commit_hash,\n\u001b[32m   4916\u001b[39m     is_remote_code=\u001b[38;5;28mcls\u001b[39m._auto_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   4917\u001b[39m     transformers_explicit_filename=transformers_explicit_filename,\n\u001b[32m   4918\u001b[39m )\n\u001b[32m   4920\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4921\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/transformers/modeling_utils.py:1037\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1023\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m   1024\u001b[39m     cached_file_kwargs = {\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m   1026\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1035\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m   1036\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n\u001b[32m   1039\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1040\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1042\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         hf_hub_download(\n\u001b[32m    480\u001b[39m             path_or_repo_id,\n\u001b[32m    481\u001b[39m             filenames[\u001b[32m0\u001b[39m],\n\u001b[32m    482\u001b[39m             subfolder=\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[32m    483\u001b[39m             repo_type=repo_type,\n\u001b[32m    484\u001b[39m             revision=revision,\n\u001b[32m    485\u001b[39m             cache_dir=cache_dir,\n\u001b[32m    486\u001b[39m             user_agent=user_agent,\n\u001b[32m    487\u001b[39m             force_download=force_download,\n\u001b[32m    488\u001b[39m             proxies=proxies,\n\u001b[32m    489\u001b[39m             resume_download=resume_download,\n\u001b[32m    490\u001b[39m             token=token,\n\u001b[32m    491\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    492\u001b[39m         )\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    991\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    992\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1007\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1008\u001b[39m     )\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[32m   1011\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m   1012\u001b[39m         cache_dir=cache_dir,\n\u001b[32m   1013\u001b[39m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[32m   1014\u001b[39m         repo_id=repo_id,\n\u001b[32m   1015\u001b[39m         filename=filename,\n\u001b[32m   1016\u001b[39m         repo_type=repo_type,\n\u001b[32m   1017\u001b[39m         revision=revision,\n\u001b[32m   1018\u001b[39m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[32m   1019\u001b[39m         endpoint=endpoint,\n\u001b[32m   1020\u001b[39m         etag_timeout=etag_timeout,\n\u001b[32m   1021\u001b[39m         headers=hf_headers,\n\u001b[32m   1022\u001b[39m         proxies=proxies,\n\u001b[32m   1023\u001b[39m         token=token,\n\u001b[32m   1024\u001b[39m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[32m   1025\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1026\u001b[39m         force_download=force_download,\n\u001b[32m   1027\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1171\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m     _download_to_tmp_and_move(\n\u001b[32m   1172\u001b[39m         incomplete_path=Path(blob_path + \u001b[33m\"\u001b[39m\u001b[33m.incomplete\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1173\u001b[39m         destination_path=Path(blob_path),\n\u001b[32m   1174\u001b[39m         url_to_download=url_to_download,\n\u001b[32m   1175\u001b[39m         proxies=proxies,\n\u001b[32m   1176\u001b[39m         headers=headers,\n\u001b[32m   1177\u001b[39m         expected_size=expected_size,\n\u001b[32m   1178\u001b[39m         filename=filename,\n\u001b[32m   1179\u001b[39m         force_download=force_download,\n\u001b[32m   1180\u001b[39m         etag=etag,\n\u001b[32m   1181\u001b[39m         xet_file_data=xet_file_data,\n\u001b[32m   1182\u001b[39m     )\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1184\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1723\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[32m   1722\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1723\u001b[39m     xet_get(\n\u001b[32m   1724\u001b[39m         incomplete_path=incomplete_path,\n\u001b[32m   1725\u001b[39m         xet_file_data=xet_file_data,\n\u001b[32m   1726\u001b[39m         headers=headers,\n\u001b[32m   1727\u001b[39m         expected_size=expected_size,\n\u001b[32m   1728\u001b[39m         displayed_filename=filename,\n\u001b[32m   1729\u001b[39m     )\n\u001b[32m   1730\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1731\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants.HF_HUB_DISABLE_XET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:624\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    613\u001b[39m     displayed_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename[:\u001b[32m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(â€¦)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    615\u001b[39m progress_cm = _get_progress_bar_context(\n\u001b[32m    616\u001b[39m     desc=displayed_filename,\n\u001b[32m    617\u001b[39m     log_level=logger.getEffectiveLevel(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m     _tqdm_bar=_tqdm_bar,\n\u001b[32m    622\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress_cm \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m    626\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m    627\u001b[39m         progress.update(progress_bytes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/tqdm/std.py:1138\u001b[39m, in \u001b[36mtqdm.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels = 3,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_encodings = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:13:03.316152Z",
     "iopub.status.busy": "2025-11-15T14:13:03.315355Z",
     "iopub.status.idle": "2025-11-15T14:13:03.354167Z",
     "shell.execute_reply": "2025-11-15T14:13:03.353199Z",
     "shell.execute_reply.started": "2025-11-15T14:13:03.316096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_ids = train_encodings['input_ids']\n",
    "attention_mask = train_encodings['attention_mask']\n",
    "train_labels = torch.tensor(y_train)\n",
    "dataset = TensorDataset(input_ids, attention_mask, train_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "input_ids = test_encodings['input_ids']\n",
    "attention_mask = test_encodings['attention_mask']\n",
    "test_labels = torch.tensor(y_test)\n",
    "test_dataset = TensorDataset(input_ids, attention_mask, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 2\n",
    "total_step = len(dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-15T14:13:40.768Z",
     "iopub.execute_input": "2025-11-15T14:13:04.042747Z",
     "iopub.status.busy": "2025-11-15T14:13:04.041758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1/2\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f'Epoch{epoch + 1}/{num_epochs}')\n",
    "    total_train_loss = 0\n",
    "    total_train_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    \n",
    "    for b_input_ids, b_mask, b_labels in dataloader:\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_mask = b_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            attention_mask = b_mask,\n",
    "            labels = b_labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_train_correct += (preds == b_labels).sum().item()\n",
    "        total_train_samples += b_labels.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    train_accuracy = total_train_correct / total_train_samples\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} complete. Avg Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b_input_ids, b_mask, b_labels in test_dataloader:\n",
    "            b_input_ids=b_input_ids.to(device)\n",
    "            b_mask=b_mask.to(device)\n",
    "            b_labels=b_labels.to(device)\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                attention_mask = b_mask\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, b_labels)\n",
    "            total_val_loss += loss\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_val_correct += (preds == b_labels).sum().item()\n",
    "            total_val_samples += b_labels.size(0)\n",
    "            \n",
    "        avg_test_loss = total_val_loss / len(test_dataloader)\n",
    "        test_accuracy = total_val_correct / total_val_samples\n",
    "        print(f\"Epoch {epoch + 1} complete. Avg Loss: {avg_test_loss:.4f}, Validtion Accuracy: {test_accuracy:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-15T09:05:07.392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for b_input_ids, b_mask, b_labels in test_dataloader:\n",
    "        b_input_ids=b_input_ids.to(device)\n",
    "        b_mask=b_mask.to(device)\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            attention_mask = b_mask\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=1)\n",
    "        y_pred.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-15T09:05:07.392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = torch.cat(y_pred)\n",
    "\n",
    "y_pred = y_pred.flatten().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-15T09:05:07.393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')    \n",
    "f1 = f1_score(y_test, y_pred, average='macro')           \n",
    "\n",
    "\n",
    "print(f\"--- Performance ---\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative (0)', 'Neutral (1)', 'Positive (2)']))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-15T09:05:07.393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_text = [\"\"]\n",
    "inputs = tokenizer(\n",
    "    test_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask']\n",
    "    )\n",
    "logits = outputs.logits\n",
    "print(logits)\n",
    "\n",
    "prediction = torch.argmax(logits, dim=1)\n",
    "print(f\"Prediction: {prediction.item()} (0=Neg, 1=Neu, 2=Pos)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "datasetId": 8388491,
     "sourceId": 13632917,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
