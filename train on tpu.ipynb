{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":13632917,"sourceType":"datasetVersion","datasetId":8388491}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nimport logging\nfrom transformers.utils import logging as hf_logging\n\nhf_logging.set_verbosity_error()\nwarnings.filterwarnings(\"ignore\", message=\".*Transparent hugepages.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T07:52:29.762155Z","iopub.execute_input":"2025-11-16T07:52:29.762389Z","iopub.status.idle":"2025-11-16T07:52:31.664464Z","shell.execute_reply.started":"2025-11-16T07:52:29.762369Z","shell.execute_reply":"2025-11-16T07:52:31.663377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, \n    confusion_matrix, \n    precision_score, \n    recall_score, \n    f1_score,\n    classification_report\n)\nimport os\nimport torch\nimport torch.nn as nn\nimport torch_xla.runtime as xr\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ.pop('TPU_PROCESS_ADDRESSES')\n\nMODEL_NAME = \"tabularisai/multilingual-sentiment-analysis\"\nLEARNING_RATE = 1e-5\nEPOCHS = 12\nBATCH_SIZE = 8\n\ntxt = pd.read_csv(\"/kaggle/input/mrbeast-youtube-comment-sentiment-analysis/sentiment_analysis_dataset.csv\", on_bad_lines='skip')\n\ntext = list(txt.iloc[:, 0].astype(\"str\"))\nlabels = list(txt.iloc[:, 1].str.strip().str.capitalize())\n\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(labels)\n\nclass_counts = np.bincount(labels)\ntotal_samples = len(labels)\nnum_classes = len(class_counts)\nclass_weights = total_samples / (num_classes * class_counts)\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    text, labels,\n    test_size=0.2,\n    random_state=42,\n    stratify=labels\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ntrain_encodings = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt')\ntest_encodings = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt')\n\ninput_ids = train_encodings['input_ids']\nattention_mask = train_encodings['attention_mask']\ntrain_labels = torch.tensor(y_train)\ntrain_dataset = TensorDataset(input_ids, attention_mask, train_labels)\n\ninput_ids = test_encodings['input_ids']\nattention_mask = test_encodings['attention_mask']\ntest_labels = torch.tensor(y_test)\ntest_dataset = TensorDataset(input_ids, attention_mask, test_labels)\n\ndef _mp_fn(index):  # 'index' is the TPU core number (0-7)\n    \n    # 1. ACQUIRE DEVICE\n    device = torch_xla.device()\n    weights_tensor = class_weights_tensor.to(device)\n    \n    # 2. MODEL LOADING (Move inside)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels = 3,\n        ignore_mismatched_sizes=True\n    )\n    model.to(device)\n\n    # 3. DATALOADERS (Modify for distributed training)\n    # Use DistributedSampler to split data across cores\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=xr.world_size(),  # Total cores (8)\n        rank=xr.global_ordinal(),            # This core's ID\n        shuffle=True\n    )\n    \n    # NOTE: Your batch_size=2 is PER CORE. Global batch size will be 2 * 8 = 16.\n    # You should increase this for better TPU performance.\n    dataloader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        sampler=train_sampler\n    )\n\n    test_sampler = DistributedSampler(\n        test_dataset,\n        num_replicas=xr.world_size(),\n        rank=xr.global_ordinal(),\n        shuffle=False  # No need to shuffle test data\n    )\n    \n    test_dataloader = DataLoader(\n        test_dataset, \n        batch_size=BATCH_SIZE, \n        sampler=test_sampler\n    )\n\n    # 4. OPTIMIZER & SCHEDULER (Move inside)\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n    criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n    num_epochs = EPOCHS\n    total_step = len(dataloader) * num_epochs # len(dataloader) is correct here\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=600,\n        num_training_steps=total_step\n    )\n\n    # 5. TRAINING LOOP (Modify 3 key parts)\n    for epoch in range(num_epochs):\n        # --- WRAP DataLoaders with ParallelLoader ---\n        p_dataloader = pl.ParallelLoader(dataloader, [device]).per_device_loader(device)\n        p_test_dataloader = pl.ParallelLoader(test_dataloader, [device]).per_device_loader(device)\n        model.train()\n        # Use xm.master_print to print only from one core\n        xm.master_print(f'Epoch{epoch + 1}/{num_epochs}')\n        total_train_loss = torch.tensor(0.0, device=device)\n        total_train_correct = torch.tensor(0, device=device, dtype=torch.long)\n        total_train_samples = torch.tensor(0, device=device, dtype=torch.long)\n        \n        # Use the ParallelLoader\n        for b_input_ids, b_mask, b_labels in p_dataloader:\n            b_input_ids = b_input_ids.to(device)\n            b_mask = b_mask.to(device)\n            b_labels = b_labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(\n                b_input_ids,\n                attention_mask = b_mask,\n                labels = b_labels\n            )\n            \n            logits = outputs.logits\n            loss = criterion(logits, b_labels)\n            total_train_loss += loss\n            preds = torch.argmax(logits, dim=1)\n            total_train_correct += (preds == b_labels).sum()\n            total_train_samples += b_labels.size(0)\n            \n            loss.backward()\n            \n            xm.optimizer_step(optimizer, barrier=True)\n\n            scheduler.step()\n\n        # --- 3. Aggregate tensors from all cores ---\n        global_train_loss = xm.all_reduce(xm.REDUCE_SUM, total_train_loss)\n        global_train_correct = xm.all_reduce(xm.REDUCE_SUM, total_train_correct)\n        global_train_samples = xm.all_reduce(xm.REDUCE_SUM, total_train_samples)\n\n        # --- 4. Calculate and report global metrics ---\n        total_train_batches = len(dataloader) * xr.world_size() \n\n        if total_train_batches > 0 and global_train_samples > 0:\n            avg_train_loss = global_train_loss / total_train_batches\n            train_accuracy = global_train_correct.float() / global_train_samples\n            xm.master_print(f\"Epoch {epoch + 1} complete. Global Avg Loss: {avg_train_loss:.4f}, Global Train Accuracy: {train_accuracy:.4f}\")\n        else:\n            xm.master_print(f\"Epoch {epoch + 1} complete. No training data.\")\n\n        \n        # 6. VALIDATION LOOP (Modify 2 parts)\n        model.eval()\n        total_val_loss = torch.tensor(0.0, device=device)\n        total_val_correct = torch.tensor(0, device=device, dtype=torch.long)\n        total_val_samples = torch.tensor(0, device=device, dtype=torch.long)\n        \n        with torch.no_grad():\n            # Use the ParallelLoader\n            for b_input_ids, b_mask, b_labels in p_test_dataloader:\n                b_input_ids=b_input_ids.to(device)\n                b_mask=b_mask.to(device)\n                b_labels=b_labels.to(device)\n                \n                outputs = model(\n                b_input_ids,\n                attention_mask = b_mask,\n                labels = b_labels\n            )\n                \n            logits = outputs.logits\n            loss = criterion(logits, b_labels)\n            total_val_loss += loss\n            preds = torch.argmax(logits, dim=1)\n            total_val_correct += (preds == b_labels).sum()\n            total_val_samples += b_labels.size(0)\n                \n        global_val_loss = xm.all_reduce(xm.REDUCE_SUM, total_val_loss)\n        global_val_correct = xm.all_reduce(xm.REDUCE_SUM, total_val_correct)\n        global_val_samples = xm.all_reduce(xm.REDUCE_SUM, total_val_samples)\n\n        # 4. Calculate global metrics\n        # len(test_dataloader) is batches_per_core, so multiply by world_size\n        total_val_batches = len(test_dataloader) * xr.world_size() \n        \n        avg_test_loss = global_val_loss / total_val_batches\n        test_accuracy = global_val_correct.float() / global_val_samples # Use .float() for precision\n        xm.master_print(f\"Epoch {epoch + 1} complete. Avg Loss: {avg_test_loss:.4f}, Validtion Accuracy: {test_accuracy:.4f}\")\n\n    # 7. FINAL METRICS (Run only on master core)\n    if xm.is_master_ordinal():\n        print(\"\\n\")\n        print(\"Saving model and tokenizer...\")\n        # Use xm.save() to correctly save from the TPU\n        xm.save(model.state_dict(), \"my_trained_model.pt\")\n        # Save the tokenizer so you can load it easily later\n        tokenizer.save_pretrained(\"./my_tokenizer_directory\")\n        print(\"Model and tokenizer saved successfully.\")\n        \nif __name__ == \"__main__\":\n    xmp.spawn(_mp_fn, args=(), nprocs=None, start_method='fork')\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T07:52:31.665382Z","iopub.execute_input":"2025-11-16T07:52:31.665660Z","iopub.status.idle":"2025-11-16T07:56:16.476116Z","shell.execute_reply.started":"2025-11-16T07:52:31.665642Z","shell.execute_reply":"2025-11-16T07:56:16.474982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Define Paths and Model Config ---\nMODEL_NAME = MODEL_NAME\nSAVED_MODEL_PATH = \"/kaggle/working/my_trained_model.pt\"\nSAVED_TOKENIZER_PATH = \"/kaggle/working/my_tokenizer_directory\"\nNUM_LABELS = 3\nLABEL_NAMES = ['Negative (0)', 'Neutral (1)', 'Positive (2)']\n\n# --- 2. Load Model and Tokenizer ---\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(SAVED_TOKENIZER_PATH)\n\nprint(\"Loading model architecture...\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, \n    num_labels=NUM_LABELS,\n    ignore_mismatched_sizes=True\n)\n\nprint(\"Loading trained weights...\")\nmodel.load_state_dict(torch.load(SAVED_MODEL_PATH))\n\n# Use GPU if available, otherwise CPU. This does NOT use TPU.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\nprint(f\"Model loaded successfully on {device}\")\n\n# --- 3. Create a SIMPLE DataLoader ---\n# We use the 'test_dataset' variable from your first script\n# NO DistributedSampler, NO ParallelLoader\neval_dataloader = DataLoader(\n    test_dataset, \n    batch_size=BATCH_SIZE  # A standard batch size is fine\n)\n\n# --- 4. Run Evaluation ---\nprint(\"Running evaluation...\")\nall_predictions = []\nwith torch.no_grad():\n    for batch in eval_dataloader:\n        # Dataloader gives (input_ids, attention_mask, labels)\n        # We only need the first two. Move them to the device.\n        b_input_ids = batch[0].to(device)\n        b_mask = batch[1].to(device)\n        \n        outputs = model(b_input_ids, attention_mask=b_mask)\n        logits = outputs.logits\n        \n        # Get predictions and move them back to CPU\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n        all_predictions.append(preds)\n\n# Flatten all predictions into a single numpy array\ny_pred = np.concatenate(all_predictions)\n\n# --- 5. Get Metrics ---\n# This will now work perfectly: len(y_pred) == 1361 and len(y_test) == 1361\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1 = f1_score(y_test, y_pred, average='macro')\n\nprint(f\"--- Performance ---\")\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\nprint(\"\\n\")\n\nprint(\"--- Classification Report ---\")\nprint(classification_report(y_test, y_pred, target_names=['Negative (0)', 'Neutral (1)', 'Positive (2)']))\nprint(\"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T07:56:16.476711Z","iopub.execute_input":"2025-11-16T07:56:16.476914Z","iopub.status.idle":"2025-11-16T07:56:27.366358Z","shell.execute_reply.started":"2025-11-16T07:56:16.476897Z","shell.execute_reply":"2025-11-16T07:56:27.365364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\ntest_text = [\"that's how you use power of money, not billion dollars cars. Bless him\"]\ninputs = tokenizer(\n    test_text,\n    padding=True,\n    truncation=True,\n    return_tensors='pt'\n).to(device)\n\nwith torch.no_grad():\n    outputs = model(\n        inputs['input_ids'],\n        attention_mask=inputs['attention_mask']\n    )\nlogits = outputs.logits\nprint(logits)\n\nprediction = torch.argmax(logits, dim=1)\nprint(f\"Prediction: {prediction.item()} (0=Neg, 1=Neu, 2=Pos)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T07:59:45.573574Z","iopub.execute_input":"2025-11-16T07:59:45.573831Z","iopub.status.idle":"2025-11-16T07:59:45.593391Z","shell.execute_reply.started":"2025-11-16T07:59:45.573813Z","shell.execute_reply":"2025-11-16T07:59:45.592768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import IPython\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T07:52:01.086078Z","iopub.execute_input":"2025-11-16T07:52:01.086342Z","iopub.status.idle":"2025-11-16T07:52:01.092583Z","shell.execute_reply.started":"2025-11-16T07:52:01.086323Z","shell.execute_reply":"2025-11-16T07:52:01.091677Z"}},"outputs":[],"execution_count":null}]}